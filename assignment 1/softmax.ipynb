{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.392909\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ * In case ofjust calculating the softmax based on some initial random weights, we expect that the initial loss has to be close to -log(0.1) because initially all the classes are equally likely to be chosen. In Dataset we have 10 classes, thus probability of the correct class will be 0.1 and the softmax loss is the negative log probability of the correct class, therefore it is -log(0.1).* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -3.010547 analytic: -3.010547, relative error: 6.425655e-09\n",
      "numerical: 0.542430 analytic: 0.542430, relative error: 2.203092e-08\n",
      "numerical: 1.022481 analytic: 1.022481, relative error: 4.277701e-08\n",
      "numerical: 0.370825 analytic: 0.370825, relative error: 1.397815e-07\n",
      "numerical: 1.167200 analytic: 1.167200, relative error: 3.132234e-08\n",
      "numerical: -1.376276 analytic: -1.376276, relative error: 1.862586e-08\n",
      "numerical: 1.270231 analytic: 1.270231, relative error: 2.590655e-08\n",
      "numerical: 2.880071 analytic: 2.880071, relative error: 6.180727e-09\n",
      "numerical: -0.753294 analytic: -0.753294, relative error: 2.340094e-08\n",
      "numerical: 2.641566 analytic: 2.641565, relative error: 3.032319e-08\n",
      "numerical: -4.642631 analytic: -4.642631, relative error: 8.054503e-09\n",
      "numerical: 1.872715 analytic: 1.872715, relative error: 2.466802e-08\n",
      "numerical: -2.188567 analytic: -2.188567, relative error: 5.420385e-09\n",
      "numerical: -0.054657 analytic: -0.054657, relative error: 3.723656e-07\n",
      "numerical: -0.670197 analytic: -0.670197, relative error: 3.845195e-08\n",
      "numerical: 1.801028 analytic: 1.801028, relative error: 2.375999e-09\n",
      "numerical: 1.602400 analytic: 1.602400, relative error: 6.341793e-09\n",
      "numerical: 0.021435 analytic: 0.021435, relative error: 1.974256e-07\n",
      "numerical: -0.052600 analytic: -0.052600, relative error: 7.571213e-07\n",
      "numerical: 1.142144 analytic: 1.142144, relative error: 2.554170e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.392909e+00 computed in 0.139635s\n",
      "vectorized loss: 2.392909e+00 computed in 0.007897s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 84432938878038720.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinkar/Downloads/assignment1_jupyter/assignment1/cs231n/classifiers/softmax.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.sum(-np.log(softmax_matrix[np.arange(num_train),y]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.087980 val accuracy: 0.090000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.104286 val accuracy: 0.110000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.072796 val accuracy: 0.082000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.096980 val accuracy: 0.102000\n",
      "best validation accuracy achieved during cross-validation: 0.110000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "grid_search = [ (lr, rg) for lr in learning_rates for rg in regularization_strengths]\n",
    "\n",
    "for lr, rg in grid_search:\n",
    "    # Create a new Softmax instance\n",
    "    softmax_model = Softmax()\n",
    "    # Train the model with current parameters\n",
    "    softmax_model.train(X_train, y_train, learning_rate=lr, reg=rg, num_iters=1000)\n",
    "    # Predict values for training set\n",
    "    y_train_pred = softmax_model.predict(X_train)\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = np.mean(y_train_pred == y_train)\n",
    "    # Predict values for validation set\n",
    "    y_val_pred = softmax_model.predict(X_val)\n",
    "    # Calculate accuracy\n",
    "    val_accuracy = np.mean(y_val_pred == y_val)\n",
    "    # Save results\n",
    "    results[(lr,rg)] = (train_accuracy, val_accuracy)\n",
    "    if best_val < val_accuracy:\n",
    "        best_val = val_accuracy\n",
    "        best_softmax = softmax_model\n",
    "    \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.116000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ True\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$ SVM classifier is happy after maintaining a margin of delta between the correct class and the incorrect class but its not the the case with softmax classifier. Softmax classifer is sensitive very much sensitive to the weight matrix and loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
